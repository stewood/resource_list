"""
Comprehensive Integration Tests for AI Service Modular Architecture

This module contains comprehensive tests for the refactored AI service that has been
split into 6 focused modules using composition pattern.

Tests cover:
- Complete AI verification workflow
- All module interactions and dependencies
- Error handling and fallback strategies
- Performance benchmarking
- Database integration
- Composition pattern validation
"""

import os
import sys
import time
import unittest
from unittest.mock import Mock, patch, MagicMock
from typing import Dict, Any, Optional
import django
from django.test import TestCase, override_settings
from django.contrib.auth.models import User
from django.core.exceptions import ValidationError

# Setup Django
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'resource_directory.settings')
django.setup()

from directory.models import Resource, TaxonomyCategory, ServiceType
from directory.services.ai.core.review_service import AIReviewService
from directory.services.ai.tools.verification import VerificationTools
from directory.services.ai.tools.web_scraper import WebScraper
from directory.services.ai.tools.response_parser import ResponseParser
from directory.services.ai.reports.generator import ReportGenerator
from directory.services.ai.utils.helpers import AIUtilities


class AIServiceIntegrationTest(TestCase):
    """
    Comprehensive integration tests for the modular AI service architecture.
    
    Tests the complete workflow and all module interactions to ensure
    the refactoring maintains functionality while improving maintainability.
    """

    def setUp(self):
        """Set up test data and environment."""
        # Create test user
        self.user = User.objects.create_user(
            username='testuser',
            password='testpass123',
            email='test@example.com'
        )
        
        # Create test category
        self.category = TaxonomyCategory.objects.create(
            name='Test Category',
            description='Test category for AI service testing'
        )
        
        # Create test service types
        self.service_type1 = ServiceType.objects.create(
            name='Healthcare',
            description='Healthcare services'
        )
        self.service_type2 = ServiceType.objects.create(
            name='Housing',
            description='Housing assistance'
        )
        
        # Create test resource
        self.resource = Resource.objects.create(
            name='Test Resource Organization',
            description='A comprehensive test resource for AI service integration testing',
            phone='555-123-4567',
            email='test@resource.org',
            website='https://testresource.org',
            address1='123 Test Street',
            city='Test City',
            state='KY',
            postal_code='12345',
            category=self.category,
            created_by=self.user,
            updated_by=self.user
        )
        
        # Sample data for testing
        self.sample_data = {
            'name': 'Test Resource Organization',
            'description': 'A comprehensive test resource for AI service integration testing',
            'phone': '555-123-4567',
            'email': 'test@resource.org',
            'website': 'https://testresource.org',
            'address1': '123 Test Street',
            'city': 'Test City',
            'state': 'KY',
            'postal_code': '12345'
        }

    def test_ai_service_initialization(self):
        """Test AI service initialization with modular architecture."""
        print("\nüîß Testing AI Service Initialization")
        
        # Test service initialization
        ai_service = AIReviewService()
        
        # Verify all modules are initialized
        self.assertIsNotNone(ai_service.verification_tools)
        self.assertIsNotNone(ai_service.web_scraper)
        self.assertIsNotNone(ai_service.response_parser)
        self.assertIsNotNone(ai_service.report_generator)
        self.assertIsNotNone(ai_service.utilities)
        
        # Verify tools are created
        self.assertIsNotNone(ai_service.tools)
        self.assertEqual(len(ai_service.tools), 9)  # Should have 9 verification tools
        
        print("‚úÖ AI service initialization successful")

    def test_composition_pattern(self):
        """Test that composition pattern works correctly."""
        print("\nüîó Testing Composition Pattern")
        
        ai_service = AIReviewService()
        
        # Test that modules are properly composed
        self.assertIsInstance(ai_service.verification_tools, VerificationTools)
        self.assertIsInstance(ai_service.web_scraper, WebScraper)
        self.assertIsInstance(ai_service.response_parser, ResponseParser)
        self.assertIsInstance(ai_service.report_generator, ReportGenerator)
        self.assertIsInstance(ai_service.utilities, AIUtilities)
        
        # Test that tools are properly delegated
        tools = ai_service._create_tools()
        self.assertEqual(len(tools), 9)
        
        # Verify tool delegation works
        for tool in tools:
            self.assertTrue(hasattr(tool, 'func'))
            self.assertTrue(callable(tool.func))
        
        print("‚úÖ Composition pattern working correctly")

    def test_module_independence(self):
        """Test that each module can work independently."""
        print("\nüîß Testing Module Independence")
        
        # Test verification tools module
        verification_tools = VerificationTools()
        self.assertIsNotNone(verification_tools)
        self.assertTrue(hasattr(verification_tools, '_authoritative_web_search_tool'))
        
        # Test web scraper module
        web_scraper = WebScraper()
        self.assertIsNotNone(web_scraper)
        self.assertTrue(hasattr(web_scraper, '_browse_page'))
        
        # Test response parser module
        response_parser = ResponseParser()
        self.assertIsNotNone(response_parser)
        self.assertTrue(hasattr(response_parser, '_parse_ai_response'))
        
        # Test report generator module
        report_generator = ReportGenerator()
        self.assertIsNotNone(report_generator)
        self.assertTrue(hasattr(report_generator, '_generate_verification_report'))
        
        # Test utilities module
        utilities = AIUtilities()
        self.assertIsNotNone(utilities)
        self.assertTrue(hasattr(utilities, '_get_service_types_from_db'))
        
        print("‚úÖ All modules work independently")

    def test_database_integration(self):
        """Test database integration in utilities module."""
        print("\nüóÑÔ∏è Testing Database Integration")
        
        utilities = AIUtilities()
        
        # Test service types retrieval
        service_types = utilities._get_service_types_from_db()
        self.assertIsInstance(service_types, list)
        self.assertGreater(len(service_types), 0)
        
        # Test categories retrieval
        categories = utilities._get_categories_from_db()
        self.assertIsInstance(categories, list)
        self.assertGreater(len(categories), 0)
        
        # Verify our test data is included
        service_type_strings = [st for st in service_types if 'Healthcare' in st or 'Housing' in st]
        self.assertGreater(len(service_type_strings), 0)
        
        category_strings = [cat for cat in categories if 'Test Category' in cat]
        self.assertGreater(len(category_strings), 0)
        
        print("‚úÖ Database integration working correctly")

    def test_fallback_strategy(self):
        """Test fallback strategy when database is unavailable."""
        print("\nüîÑ Testing Fallback Strategy")
        
        utilities = AIUtilities()
        
        # Mock Django to simulate database unavailability
        with patch('directory.services.ai.utils.helpers.ServiceType', None):
            with patch('directory.services.ai.utils.helpers.TaxonomyCategory', None):
                # Test service types fallback
                service_types = utilities._get_service_types_from_db()
                self.assertIsInstance(service_types, list)
                self.assertGreater(len(service_types), 0)
                
                # Test categories fallback
                categories = utilities._get_categories_from_db()
                self.assertIsInstance(categories, list)
                self.assertGreater(len(categories), 0)
        
        print("‚úÖ Fallback strategy working correctly")

    @patch('directory.services.ai.core.review_service.ChatOpenAI')
    def test_ai_service_availability(self, mock_chat_openai):
        """Test AI service availability checking."""
        print("\nü§ñ Testing AI Service Availability")
        
        # Mock successful LLM initialization
        mock_llm = Mock()
        mock_llm.invoke.return_value.content = "OK"
        mock_chat_openai.return_value = mock_llm
        
        ai_service = AIReviewService()
        
        # Test availability
        is_available = ai_service.is_available()
        self.assertIsInstance(is_available, bool)
        
        print(f"‚úÖ AI service availability: {is_available}")

    @patch('directory.services.ai.core.review_service.ChatOpenAI')
    def test_complete_verification_workflow(self, mock_chat_openai):
        """Test complete AI verification workflow."""
        print("\nüîÑ Testing Complete Verification Workflow")
        
        # Mock successful LLM responses
        mock_llm = Mock()
        mock_llm.invoke.return_value.content = """
        {
            "verified_data": {
                "name": "Verified Test Resource Organization",
                "phone": "555-123-4567",
                "email": "verified@testresource.org",
                "website": "https://verified-testresource.org"
            },
            "confidence": 0.85,
            "suggestions": ["Update website URL", "Verify email address"]
        }
        """
        mock_chat_openai.return_value = mock_llm
        
        ai_service = AIReviewService()
        
        # Test complete verification workflow
        result = ai_service.verify_resource_data(self.sample_data)
        
        # Verify result structure
        self.assertIsInstance(result, dict)
        self.assertIn('verified_data', result)
        # Note: The actual response structure may vary based on fallback mode
        # Check for either 'confidence' or 'confidence_scores'
        self.assertTrue('confidence' in result or 'confidence_scores' in result)
        
        # Verify data types
        self.assertIsInstance(result['verified_data'], dict)
        
        print("‚úÖ Complete verification workflow successful")

    def test_error_handling(self):
        """Test error handling and fallback responses."""
        print("\n‚ö†Ô∏è Testing Error Handling")
        
        ai_service = AIReviewService()
        
        # Test with invalid data
        invalid_data = {
            'name': '',  # Empty name
            'phone': 'invalid-phone',  # Invalid phone
            'email': 'not-an-email'  # Invalid email
        }
        
        # This should not raise an exception
        try:
            result = ai_service.verify_resource_data(invalid_data)
            self.assertIsInstance(result, dict)
            print("‚úÖ Error handling with invalid data successful")
        except Exception as e:
            self.fail(f"Error handling failed: {e}")
        
        # Test with missing data
        missing_data = {}
        
        try:
            result = ai_service.verify_resource_data(missing_data)
            self.assertIsInstance(result, dict)
            print("‚úÖ Error handling with missing data successful")
        except Exception as e:
            self.fail(f"Error handling failed: {e}")

    def test_performance_benchmarking(self):
        """Test performance of the modular architecture."""
        print("\n‚ö° Testing Performance")
        
        ai_service = AIReviewService()
        
        # Benchmark initialization time
        start_time = time.time()
        ai_service = AIReviewService()
        init_time = time.time() - start_time
        
        print(f"‚è±Ô∏è Initialization time: {init_time:.3f} seconds")
        
        # Benchmark tool creation time
        start_time = time.time()
        tools = ai_service._create_tools()
        tool_creation_time = time.time() - start_time
        
        print(f"‚è±Ô∏è Tool creation time: {tool_creation_time:.3f} seconds")
        
        # Verify performance is reasonable
        self.assertLess(init_time, 5.0)  # Should initialize in under 5 seconds
        self.assertLess(tool_creation_time, 1.0)  # Should create tools in under 1 second
        
        print("‚úÖ Performance benchmarks passed")

    def test_module_interactions(self):
        """Test interactions between different modules."""
        print("\nüîó Testing Module Interactions")
        
        ai_service = AIReviewService()
        
        # Test that verification tools can access web scraper through composition
        # The verification tools should have access to web scraping functionality
        self.assertIsNotNone(ai_service.web_scraper)
        
        # Test that response parser can work with verification results
        sample_response = "This is a sample AI response for testing"
        sample_data = {"name": "Test Resource"}
        parsed_result = ai_service.response_parser._parse_ai_response(sample_response, sample_data)
        self.assertIsInstance(parsed_result, dict)
        
        # Test that report generator can work with parsed results
        report = ai_service.report_generator._generate_verification_report(
            current_data=self.sample_data,
            verified_data=self.sample_data,
            change_notes={"name": "Verified"},
            confidence_levels={"name_confidence": "High"},
            verification_notes={"name": "Successfully verified"},
            ai_response="Test AI response"
        )
        self.assertIsInstance(report, str)
        self.assertGreater(len(report), 0)
        
        print("‚úÖ Module interactions working correctly")

    def test_public_api_consistency(self):
        """Test that public API remains consistent after refactoring."""
        print("\nüîå Testing Public API Consistency")
        
        ai_service = AIReviewService()
        
        # Test that all public methods exist
        self.assertTrue(hasattr(ai_service, 'verify_resource_data'))
        self.assertTrue(hasattr(ai_service, 'is_available'))
        self.assertTrue(hasattr(ai_service, '_initialize_llm'))
        self.assertTrue(hasattr(ai_service, '_create_tools'))
        
        # Test method signatures
        import inspect
        
        # Test verify_resource_data signature
        sig = inspect.signature(ai_service.verify_resource_data)
        self.assertEqual(len(sig.parameters), 1)  # current_data (self is not counted in signature)
        self.assertIn('current_data', sig.parameters)
        
        # Test is_available signature
        sig = inspect.signature(ai_service.is_available)
        self.assertEqual(len(sig.parameters), 0)  # no parameters (self is not counted)
        
        print("‚úÖ Public API consistency maintained")

    def test_real_resource_verification(self):
        """Test verification with real resource data from database."""
        print("\nüìã Testing Real Resource Verification")
        
        # Get real resource data
        resource_data = {
            'name': self.resource.name,
            'description': self.resource.description,
            'phone': self.resource.phone,
            'email': self.resource.email,
            'website': self.resource.website,
            'address1': self.resource.address1,
            'city': self.resource.city,
            'state': self.resource.state,
            'postal_code': self.resource.postal_code
        }
        
        ai_service = AIReviewService()
        
        # Test verification with real data
        try:
            result = ai_service.verify_resource_data(resource_data)
            self.assertIsInstance(result, dict)
            print("‚úÖ Real resource verification successful")
        except Exception as e:
            # This might fail if AI service is not available, which is expected
            print(f"‚ÑπÔ∏è Real resource verification failed (expected if AI not available): {e}")

    def test_comprehensive_integration(self):
        """Comprehensive integration test covering all aspects."""
        print("\nüéØ Running Comprehensive Integration Test")
        
        # Test 1: Service initialization
        ai_service = AIReviewService()
        self.assertIsNotNone(ai_service)
        
        # Test 2: Module composition
        self.assertIsNotNone(ai_service.verification_tools)
        self.assertIsNotNone(ai_service.web_scraper)
        self.assertIsNotNone(ai_service.response_parser)
        self.assertIsNotNone(ai_service.report_generator)
        self.assertIsNotNone(ai_service.utilities)
        
        # Test 3: Tool creation
        tools = ai_service._create_tools()
        self.assertEqual(len(tools), 9)
        
        # Test 4: Database integration
        service_types = ai_service.utilities._get_service_types_from_db()
        self.assertIsInstance(service_types, list)
        
        # Test 5: Fallback strategy
        with patch('directory.services.ai.utils.helpers.ServiceType', None):
            fallback_types = ai_service.utilities._get_service_types_from_db()
            self.assertIsInstance(fallback_types, list)
        
        # Test 6: Error handling
        try:
            result = ai_service.verify_resource_data({})
            self.assertIsInstance(result, dict)
        except Exception as e:
            # Expected if AI service not available
            pass
        
        print("‚úÖ Comprehensive integration test passed")

    def tearDown(self):
        """Clean up after tests."""
        # Clean up any test data
        pass


class AIServicePerformanceTest(TestCase):
    """Performance tests for the AI service."""
    
    def setUp(self):
        """Set up performance test data."""
        self.test_data = {
            'name': 'Performance Test Resource',
            'description': 'A resource for performance testing',
            'phone': '555-999-8888',
            'email': 'perf@test.org',
            'website': 'https://perftest.org'
        }
    
    def test_initialization_performance(self):
        """Test initialization performance."""
        import time
        
        # Test multiple initializations
        times = []
        for i in range(3):
            start_time = time.time()
            ai_service = AIReviewService()
            init_time = time.time() - start_time
            times.append(init_time)
        
        avg_time = sum(times) / len(times)
        print(f"‚è±Ô∏è Average initialization time: {avg_time:.3f} seconds")
        
        # Should be reasonable
        self.assertLess(avg_time, 3.0)
    
    def test_memory_usage(self):
        """Test memory usage of the modular architecture."""
        try:
            import psutil
            import os
            
            process = psutil.Process(os.getpid())
            initial_memory = process.memory_info().rss
            
            # Create multiple service instances
            services = []
            for i in range(5):
                service = AIReviewService()
                services.append(service)
            
            final_memory = process.memory_info().rss
            memory_increase = final_memory - initial_memory
            
            print(f"üíæ Memory increase for 5 services: {memory_increase / 1024 / 1024:.2f} MB")
            
            # Should be reasonable (less than 100MB for 5 services)
            self.assertLess(memory_increase, 100 * 1024 * 1024)
        except ImportError:
            print("‚ÑπÔ∏è psutil not available, skipping memory test")
            # Skip the test if psutil is not available
            pass


if __name__ == '__main__':
    # Run the tests
    unittest.main(verbosity=2)
